"""
File and Hash Analysis Module
State 1: File and Hash Analysis for Malware Agent
Extracts file hashes, executable paths, and process information from Microsoft Defender for Endpoint alerts
"""

import logging
import hashlib
import os
import struct
import pefile
from typing import Dict, Any, List, Optional, Set
from datetime import datetime
import json
import asyncio
import requests
from pathlib import Path

logger = logging.getLogger(__name__)

class FileHashAnalyzer:
    """
    File and Hash Analysis for malware detection
    Extracts and analyzes file metadata, hashes, and executable characteristics
    """
    
    def __init__(self):
        self.supported_algorithms = ['md5', 'sha1', 'sha256', 'sha512']
        self.pe_analysis_enabled = True
        self.yara_rules_path = "rules/"
        
    async def analyze_file_hashes(self, defender_alert: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze file hashes and metadata from Defender alerts
        
        Args:
            defender_alert: Microsoft Defender for Endpoint alert data
            
        Returns:
            File analysis results with hashes and metadata
        """
        logger.info("Starting file hash analysis")
        
        analysis_results = {
            "file_metadata": {},
            "hash_analysis": {},
            "pe_analysis": {},
            "process_information": {},
            "file_system_context": {},
            "detection_metadata": {},
            "extraction_timestamp": datetime.now()
        }
        
        try:
            # Extract file information from alert
            file_info = await self._extract_file_information(defender_alert)
            analysis_results["file_metadata"] = file_info
            
            # Generate file hashes
            if file_info.get("file_path"):
                hash_results = await self._generate_file_hashes(file_info["file_path"])
                analysis_results["hash_analysis"] = hash_results
                
                # PE analysis for executables
                if self._is_executable_file(file_info["file_path"]):
                    pe_results = await self._analyze_pe_structure(file_info["file_path"])
                    analysis_results["pe_analysis"] = pe_results
            
            # Extract process context
            process_info = await self._extract_process_information(defender_alert)
            analysis_results["process_information"] = process_info
            
            # File system context
            fs_context = await self._analyze_file_system_context(file_info)
            analysis_results["file_system_context"] = fs_context
            
            # Detection metadata
            detection_meta = await self._extract_detection_metadata(defender_alert)
            analysis_results["detection_metadata"] = detection_meta
            
            logger.info(f"File hash analysis completed for {len(analysis_results.get('hash_analysis', {}))} hashes")
            
        except Exception as e:
            logger.error(f"Error in file hash analysis: {str(e)}")
            analysis_results["error"] = str(e)
            
        return analysis_results
    
    async def _extract_file_information(self, defender_alert: Dict[str, Any]) -> Dict[str, Any]:
        """Extract basic file information from Defender alert"""
        file_info = {}
        
        # Extract from alert entities
        entities = defender_alert.get("entities", [])
        for entity in entities:
            if entity.get("type") == "file":
                file_info.update({
                    "file_name": entity.get("fileName"),
                    "file_path": entity.get("filePath"),
                    "file_size": entity.get("fileSize"),
                    "creation_time": entity.get("creationTimeUtc"),
                    "modification_time": entity.get("lastModificationTimeUtc"),
                    "file_directory": entity.get("directory")
                })
        
        # Extract from process information
        for entity in entities:
            if entity.get("type") == "process":
                if entity.get("imageFile"):
                    file_info.update({
                        "process_file_name": entity["imageFile"].get("fileName"),
                        "process_file_path": entity["imageFile"].get("filePath"),
                        "process_file_size": entity["imageFile"].get("fileSize")
                    })
        
        return file_info
    
    async def _generate_file_hashes(self, file_path: str) -> Dict[str, Any]:
        """Generate multiple hash types for file"""
        hash_results = {
            "file_path": file_path,
            "hashes": {},
            "hash_generation_time": datetime.now()
        }
        
        try:
            # Check if file exists and is accessible
            if not os.path.exists(file_path):
                hash_results["error"] = "File not found"
                return hash_results
            
            # Generate hashes
            with open(file_path, 'rb') as f:
                file_data = f.read()
                
                for algorithm in self.supported_algorithms:
                    if algorithm == 'md5':
                        hash_obj = hashlib.md5()
                    elif algorithm == 'sha1':
                        hash_obj = hashlib.sha1()
                    elif algorithm == 'sha256':
                        hash_obj = hashlib.sha256()
                    elif algorithm == 'sha512':
                        hash_obj = hashlib.sha512()
                    
                    hash_obj.update(file_data)
                    hash_results["hashes"][algorithm] = hash_obj.hexdigest()
            
            # Additional hash metadata
            hash_results["file_size_bytes"] = len(file_data)
            hash_results["hash_algorithms_used"] = self.supported_algorithms
            
        except Exception as e:
            hash_results["error"] = f"Hash generation failed: {str(e)}"
            logger.error(f"Failed to generate hashes for {file_path}: {str(e)}")
        
        return hash_results
    
    async def _analyze_pe_structure(self, file_path: str) -> Dict[str, Any]:
        """Analyze PE (Portable Executable) structure"""
        pe_analysis = {
            "is_pe_file": False,
            "pe_metadata": {},
            "sections": [],
            "imports": [],
            "exports": [],
            "resources": [],
            "analysis_timestamp": datetime.now()
        }
        
        try:
            if not self._is_pe_file(file_path):
                pe_analysis["error"] = "Not a valid PE file"
                return pe_analysis
            
            pe = pefile.PE(file_path)
            pe_analysis["is_pe_file"] = True
            
            # Basic PE metadata
            pe_analysis["pe_metadata"] = {
                "machine_type": hex(pe.FILE_HEADER.Machine),
                "number_of_sections": pe.FILE_HEADER.NumberOfSections,
                "timestamp": pe.FILE_HEADER.TimeDateStamp,
                "characteristics": hex(pe.FILE_HEADER.Characteristics),
                "entry_point": hex(pe.OPTIONAL_HEADER.AddressOfEntryPoint),
                "image_base": hex(pe.OPTIONAL_HEADER.ImageBase),
                "file_alignment": pe.OPTIONAL_HEADER.FileAlignment,
                "section_alignment": pe.OPTIONAL_HEADER.SectionAlignment
            }
            
            # Section analysis
            for section in pe.sections:
                section_info = {
                    "name": section.Name.decode('utf-8', errors='ignore').strip('\x00'),
                    "virtual_address": hex(section.VirtualAddress),
                    "virtual_size": section.Misc_VirtualSize,
                    "raw_size": section.SizeOfRawData,
                    "characteristics": hex(section.Characteristics),
                    "entropy": section.get_entropy()
                }
                pe_analysis["sections"].append(section_info)
            
            # Import analysis
            if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
                for entry in pe.DIRECTORY_ENTRY_IMPORT:
                    dll_imports = {
                        "dll_name": entry.dll.decode('utf-8', errors='ignore'),
                        "functions": []
                    }
                    for imp in entry.imports:
                        if imp.name:
                            dll_imports["functions"].append({
                                "name": imp.name.decode('utf-8', errors='ignore'),
                                "ordinal": imp.ordinal,
                                "address": hex(imp.address)
                            })
                    pe_analysis["imports"].append(dll_imports)
            
            # Export analysis
            if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):
                for exp in pe.DIRECTORY_ENTRY_EXPORT.symbols:
                    export_info = {
                        "name": exp.name.decode('utf-8', errors='ignore') if exp.name else None,
                        "ordinal": exp.ordinal,
                        "address": hex(exp.address)
                    }
                    pe_analysis["exports"].append(export_info)
            
            # Resource analysis
            if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
                pe_analysis["resources"] = self._analyze_pe_resources(pe.DIRECTORY_ENTRY_RESOURCE)
            
            pe.close()
            
        except Exception as e:
            pe_analysis["error"] = f"PE analysis failed: {str(e)}"
            logger.error(f"Failed to analyze PE structure for {file_path}: {str(e)}")
        
        return pe_analysis
    
    def _is_executable_file(self, file_path: str) -> bool:
        """Check if file is an executable"""
        executable_extensions = ['.exe', '.dll', '.sys', '.scr', '.com', '.bat', '.cmd', '.ps1']
        return any(file_path.lower().endswith(ext) for ext in executable_extensions)
    
    def _is_pe_file(self, file_path: str) -> bool:
        """Check if file is a valid PE file"""
        try:
            with open(file_path, 'rb') as f:
                # Check DOS header
                dos_header = f.read(2)
                if dos_header != b'MZ':
                    return False
                
                # Skip to PE header offset
                f.seek(60)
                pe_offset = struct.unpack('<L', f.read(4))[0]
                
                # Check PE signature
                f.seek(pe_offset)
                pe_signature = f.read(4)
                return pe_signature == b'PE\x00\x00'
        except:
            return False
    
    def _analyze_pe_resources(self, resource_entry) -> List[Dict[str, Any]]:
        """Analyze PE resources"""
        resources = []
        
        try:
            for resource_type in resource_entry.entries:
                for resource_id in resource_type.entries:
                    for resource_lang in resource_id.entries:
                        resource_info = {
                            "type": resource_type.id,
                            "id": resource_id.id,
                            "language": resource_lang.id,
                            "size": resource_lang.data.struct.Size,
                            "offset": resource_lang.data.struct.OffsetToData
                        }
                        resources.append(resource_info)
        except Exception as e:
            logger.warning(f"Resource analysis failed: {str(e)}")
        
        return resources
    
    async def _extract_process_information(self, defender_alert: Dict[str, Any]) -> Dict[str, Any]:
        """Extract process information from Defender alert"""
        process_info = {}
        
        entities = defender_alert.get("entities", [])
        for entity in entities:
            if entity.get("type") == "process":
                process_info.update({
                    "process_id": entity.get("processId"),
                    "process_name": entity.get("fileName"),
                    "command_line": entity.get("commandLine"),
                    "parent_process_id": entity.get("parentProcessId"),
                    "parent_process_name": entity.get("parentProcessFileName"),
                    "creation_time": entity.get("creationTimeUtc"),
                    "user_account": entity.get("accountName"),
                    "integrity_level": entity.get("integrityLevel"),
                    "elevation_token": entity.get("elevationToken")
                })
        
        return process_info
    
    async def _analyze_file_system_context(self, file_info: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze file system context"""
        fs_context = {}
        
        file_path = file_info.get("file_path")
        if file_path:
            path_obj = Path(file_path)
            
            fs_context = {
                "directory": str(path_obj.parent),
                "filename": path_obj.name,
                "extension": path_obj.suffix,
                "is_system_directory": self._is_system_directory(str(path_obj.parent)),
                "is_temp_directory": self._is_temp_directory(str(path_obj.parent)),
                "is_user_directory": self._is_user_directory(str(path_obj.parent)),
                "directory_writable": os.access(str(path_obj.parent), os.W_OK) if os.path.exists(str(path_obj.parent)) else False
            }
        
        return fs_context
    
    def _is_system_directory(self, directory: str) -> bool:
        """Check if directory is a system directory"""
        system_dirs = [
            'c:\\windows\\system32',
            'c:\\windows\\syswow64',
            'c:\\windows',
            'c:\\program files',
            'c:\\program files (x86)'
        ]
        return any(directory.lower().startswith(sys_dir) for sys_dir in system_dirs)
    
    def _is_temp_directory(self, directory: str) -> bool:
        """Check if directory is a temporary directory"""
        temp_indicators = ['temp', 'tmp', 'temporary']
        return any(indicator in directory.lower() for indicator in temp_indicators)
    
    def _is_user_directory(self, directory: str) -> bool:
        """Check if directory is in user profile"""
        user_indicators = ['users', 'documents', 'desktop', 'downloads', 'appdata']
        return any(indicator in directory.lower() for indicator in user_indicators)
    
    async def _extract_detection_metadata(self, defender_alert: Dict[str, Any]) -> Dict[str, Any]:
        """Extract detection metadata from alert"""
        detection_metadata = {
            "alert_id": defender_alert.get("id"),
            "alert_title": defender_alert.get("title"),
            "severity": defender_alert.get("severity"),
            "category": defender_alert.get("category"),
            "detection_source": defender_alert.get("detectionSource"),
            "service_source": defender_alert.get("serviceSource"),
            "first_activity_time": defender_alert.get("firstActivityTimeUtc"),
            "last_activity_time": defender_alert.get("lastActivityTimeUtc"),
            "alert_creation_time": defender_alert.get("alertCreationTimeUtc"),
            "machine_id": defender_alert.get("machineId"),
            "computer_dns_name": defender_alert.get("computerDnsName"),
            "threat_family_name": defender_alert.get("threatFamilyName"),
            "mitre_techniques": defender_alert.get("mitreTechniques", [])
        }
        
        return detection_metadata

# Factory function
def create_file_hash_analyzer() -> FileHashAnalyzer:
    """Create and return FileHashAnalyzer instance"""
    return FileHashAnalyzer()
